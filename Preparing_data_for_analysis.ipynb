{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preparing_data_for_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOWzUvzfsJfpn+GiWOOC3Je",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njaincode/PDA_portfolio/blob/main/Preparing_data_for_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdU9sVwwYyTJ"
      },
      "source": [
        "# Removing duplicate records from a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ea3l2qebB0_"
      },
      "source": [
        "## Check and remove NULL/NA values\n",
        "\n",
        "\n",
        "> The function *check_null_values* below use pandas dataframe function isna() to detect any missing values.\n",
        "\n",
        "> df.isna() returns a boolean same-sized object indicating if the values are NA. \n",
        "\n",
        "> The function *remove_null_values* below use dropna() to remove missing values in a dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9d4CaSgZGxw"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def check_null_values():\n",
        "  csv_url = 'https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv'\n",
        "\n",
        "  df_london_housing = pd.read_csv(csv_url)\n",
        "\n",
        "  # 1. Check if any NA values exist in the dataframe and print the result\n",
        "  # isnull is an alias of isna, so both can be used interchangably\n",
        "  if (df_london_housing.isna().values.any() == True):\n",
        "    print(f'Data frame contains NULL entries\\n')\n",
        "  else:\n",
        "    print(f'Data frame does not contain any NULL entries\\n')\n",
        "\n",
        "  # 2. Use df.info() to see which columns have null entries\n",
        "  # Used panda function instead\n",
        "  print(df_london_housing.columns[df_london_housing.isna().any()].tolist())\n",
        "\n",
        "  # isna().any() returns an index\n",
        "  print(df_london_housing.columns[df_london_housing.isna().any()])\n",
        "\n",
        "  # Different way to look at data\n",
        "  print(\"No. of columns containing null values\")\n",
        "  # isna.any -> if NA in any cell\n",
        "  print(len(df_london_housing.columns[df_london_housing.isna().any()]))\n",
        "\n",
        "  print(\"No. of columns not containing null values\")\n",
        "  # notna.all -> No NA in any cell\n",
        "  print(len(df_london_housing.columns[df_london_housing.notna().all()]))\n",
        "\n",
        "  print(\"Total no. of columns in the dataframe\")\n",
        "  print(len(df_london_housing.columns))\n",
        "\n",
        "def remove_null_values():\n",
        "  csv_url = 'https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv'\n",
        "\n",
        "  df_london_housing = pd.read_csv(csv_url)\n",
        "\n",
        "  # 1. Check if any NA values exist in life_satisfaction column\n",
        "  if (df_london_housing[\"life_satisfaction\"].isna().values.any() == True):\n",
        "    print(f'life_satisfaction column contains NULL entries\\n')\n",
        "\n",
        "    print(f'Number of rows before dropna = {len(df_london_housing.index)}')\n",
        "\n",
        "    # For specific columns, assign to another variable\n",
        "    df_nona_life_satisfaction = df_london_housing[\"life_satisfaction\"].dropna()\n",
        "    print(f'Number of rows after dropna = {len(df_nona_life_satisfaction.index)}')\n",
        "    \n",
        "  # 2. Remove all NA values across whole dataframe\n",
        "  if (df_london_housing.isna().values.any() == True):\n",
        "    print(f'Dataframe contains NULL entries\\n')\n",
        "    total_na_cells = df_london_housing.isna().sum()\n",
        "    print(f'Total number of cells per col with NA = {total_na_cells}\\n')\n",
        "\n",
        "    df_london_housing.dropna(inplace=True)\n",
        "    print(f'Number of rows after dropna on whole df = {len(df_london_housing.index)}\\n')\n",
        "\n",
        "    \n",
        "check_null_values()\n",
        "remove_null_values()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsD6OEIXYznO"
      },
      "source": [
        "## Check and remove duplicate values\n",
        "\n",
        "\n",
        "\n",
        "> The function *remove_duplicates* below uses pandas dataframe function drop_duplicates() which returns a pandas dataframe with duplicate rows removed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alGLs4LAavdL"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def remove_duplicates():\n",
        "  csv_url = 'https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv'\n",
        "\n",
        "  df_london_housing = pd.read_csv(csv_url)\n",
        "\n",
        "  # Remove duplicate area entries keeping first instance\n",
        "  print(f'Number of rows before dropna = {len(df_london_housing[\"area\"].index)}')\n",
        "  print(df_london_housing[[\"area\", \"date\"]])\n",
        "\n",
        "  # Find number of duplicate entries in column area\n",
        "  total_num_duplicates_in_area = len(df_london_housing['area']) -len(df_london_housing['area'].drop_duplicates())\n",
        "  print(f'Total num of duplicates entries in area = {total_num_duplicates_in_area}')\n",
        "\n",
        "  # Remove entries\n",
        "  df = df_london_housing[\"area\"].drop_duplicates()\n",
        "  print(f'Total num of unique entries in area = {len(df.index)}')\n",
        "  print(df)\n",
        "\n",
        "  # df_london_housing[\"area\"].drop_dup.. with inplace=True will not work! Use subset\n",
        "  df_london_housing.drop_duplicates(subset=\"area\", keep='first', inplace=True)\n",
        "  print(df_london_housing[[\"area\", \"date\"]])\n",
        "  num_rows_in_area = len(df_london_housing[\"area\"].index)\n",
        "  print(num_rows_in_area)\n",
        "\n",
        "\n",
        "remove_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmBGM8s2fosX"
      },
      "source": [
        "# Date formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL25W-HC0Bk8"
      },
      "source": [
        "## Date formatting using strftime() method\n",
        "\n",
        "The functions below demonstrate manipulation of dates using datetime and timedelta libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwid7bhGynaa"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Input any date from user and format it using strftime\n",
        "\n",
        "def date_formatting():\n",
        "  user_date, user_month, user_year = input('Please enter any date as DD MM YYYY ').split()\n",
        "  # Convert from string to integer\n",
        "  user_date = int(user_date)\n",
        "  user_month = int(user_month)\n",
        "  user_year = int(user_year)\n",
        "  \n",
        "  # Create datetime object\n",
        "  x = datetime(user_year, user_month, user_date)\n",
        "\n",
        "  # Different formats using strftime method \n",
        "  print(f' Full year {x.strftime(\"%Y\")} ')\n",
        "  print(f' Month(number) {x.strftime(\"%m\")} ')\n",
        "  print(f' Month(name) {x.strftime(\"%B\")} ')\n",
        "  print(f' Week number {x.strftime(\"%U\")} ')\n",
        "  print(f' Weekday(number) {x.strftime(\"%w\")} ')\n",
        "  print(f' Day of year {x.strftime(\"%j\")} ')\n",
        "  print(f' Day of month {x.strftime(\"%d\")} ')\n",
        "  print(f' Day of week {x.strftime(\"%a\")} ')\n",
        "\n",
        "date_formatting()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEMuw-Kf0IoI"
      },
      "source": [
        "## Date manipulation using timedelta() function\n",
        "\n",
        "The function below prints the dates of following 6 days. It uses timedelta function to calculate next date."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CoT-lqi0V71"
      },
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "def date_manipulation():\n",
        "  user_date, user_month, user_year = input('Please enter any date as DD MM YYYY ').split()\n",
        "  user_date = int(user_date)\n",
        "  user_month = int(user_month)\n",
        "  user_year = int(user_year)\n",
        "\n",
        "  # Format the user input date\n",
        "  x = datetime(user_year, user_month, user_date)\n",
        "\n",
        "  # Generate the dates of the following 6 days\n",
        "  for i in range(6):\n",
        "    fdate = x + timedelta(days=i+1)\n",
        "    print(fdate.strftime(\"%a\"), fdate.strftime(\"%B\"), fdate.strftime(\"%Y\"))\n",
        "\n",
        "\n",
        "date_manipulation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAEH7EUShnuQ"
      },
      "source": [
        "# Data wrangling (format, merge and join)\n",
        "\n",
        "\n",
        "> The functions below align the keys/column names across different dataframe by copying column and removing obsolete columns.\n",
        "> It used pandas concat and append functions to join multiple data frames which are from different sources.\n",
        "> It used df.sort_values function to sort the final values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNhB4CGPidFr"
      },
      "source": [
        "## Data wrangling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dsswRCDiCBM"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def clean_state(df):\n",
        "  # convert the values in the Pop column to numbers of 1000s from actual numbers (rounded)\n",
        "  df['Pop'] = (df['Pop']/1000).round()\n",
        "  # convert the values in the 'Pop' column to int64\n",
        "  df['Pop'] = df['Pop'].astype(int)\n",
        "  return df\n",
        "\n",
        "def clean_county(df):\n",
        "  # Add a new column called 'Pop' to the county_level_df dataframe\n",
        "  df['Pop'] = df['Population']\n",
        "  # Drop the 'Population' column \n",
        "  df.drop('Population', axis=1, inplace=True)\n",
        "  return df\n",
        "\n",
        "def data_wrangle():\n",
        "  excel_url = 'https://github.com/futureCodersSE/working-with-data/blob/main/Data%20sets/Income-Data.xlsx?raw=true'\n",
        "  \n",
        "  # 1. Read the three sheets into the dataframes\n",
        "  county_level_df = pd.read_excel(excel_url, sheet_name='county-level')\n",
        "  state_level_df = pd.read_excel(excel_url, sheet_name='state-level')\n",
        "  income_new_df = pd.read_excel(excel_url, sheet_name='income')\n",
        "\n",
        "  # To get an idea of dataframe\n",
        "  print(f'COUNTY \\n ')\n",
        "  print(county_level_df.info())\n",
        "  print(f'STATE \\n ')\n",
        "  print(county_level_df.info())\n",
        "  print(f'INCOME \\n ')\n",
        "  print(county_level_df.info())\n",
        "\n",
        "  # 2. Align the column name 'Population' to 'Pop'\n",
        "  clean_county(county_level_df)\n",
        "  \n",
        "  # 3. Format the 'Pop' column to represent values in 1000's and as integer\n",
        "  clean_state(state_level_df)\n",
        "\n",
        "  # 4. Combine county and state level data frame\n",
        "  # ignore_index set to True to have continuous indexing in final df\n",
        "  # join='inner' to concatenate columns with same name \n",
        "  df_to_concat = [county_level_df, state_level_df]\n",
        "  county_state_combined_df = pd.concat(df_to_concat, ignore_index=True, join='inner')\n",
        "  \n",
        "  # 5. Append income_new_df to the combined dataframe\n",
        "  # .append is the specific case of concat(axis=0, join='outer') \n",
        "  county_state_income_combined_df = county_state_combined_df.append(income_new_df, ignore_index=True)\n",
        "  \n",
        "  # 6. Sort the final dataframe on Income in descending order\n",
        "  county_state_income_combined_df.sort_values(by='Income', ascending=False, inplace=True)\n",
        "  \n",
        "  # Print the final dataframe\n",
        "  print(county_state_income_combined_df.shape)\n",
        "  print(county_state_income_combined_df.head(5))\n",
        "  print(county_state_income_combined_df.tail(5))\n",
        " \n",
        "data_wrangle()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymy47aiwga6n"
      },
      "source": [
        "# Normalising values\n",
        "\n",
        "\n",
        "\n",
        "> The function *normalise_total_vaccinations* below normalises the *total_vaccinations* across countries with respect to United Kingdom values.\n",
        "After normalisation, the countries whose *total_vaccinations* >= min of UK will have value 1 and others will have value of 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9wzVzIYghwR"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def df_normalise(val, **kwds):\n",
        "  min_val = kwds['uk_min']\n",
        "  if val >= min_val:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def normalise_total_vaccinations():\n",
        "  excel_url = 'https://github.com/lilaceri/Working-with-data-/blob/342abab10d93c4bf23b5c55a50f189f12a137c5f/Data%20Sets%20for%20code%20divisio/Covid%20Vaccination%20Data.xlsx?raw=true'\n",
        "  \n",
        "  # 1. Read Covid vaccination data from the by_country sheet\n",
        "  country_vaccine_df = pd.read_excel(excel_url, sheet_name='by_country')\n",
        "     \n",
        "  # 1. Find the minimum total vaccinations for the United Kingdom\n",
        "  # 2. Save this value in a variable rounded down to an integer\n",
        "  filter_uk = country_vaccine_df['country'] == 'United Kingdom'\n",
        "  # df_uk is a series\n",
        "  df_uk = country_vaccine_df.loc[filter_uk, 'total_vaccinations']\n",
        "  #print(df_uk)\n",
        "  # min_total_vaccinations_uk is dtype float64\n",
        "  min_total_vaccinations_uk = int(df_uk.min())\n",
        "  print(f'minimum total vaccinations for the United Kingdom = {min_total_vaccinations_uk}')\n",
        "  \n",
        "  # 3. Write a function to normalise total_vaccinations column so that all values less than the UK's min are 0 and all values greater than or equal to the UK's min are coded as 1\n",
        "  # Get country and total_vaccinations\n",
        "  # Type is DataFrame\n",
        "  df_country_total_vaccinations = country_vaccine_df[['country', 'total_vaccinations']]\n",
        "  #print(type(df_country_total_vaccinations))\n",
        "\n",
        "  # Remove all NaN and reset the index\n",
        "  df_country_total_vaccinations.dropna(inplace=True)\n",
        "  df_country_total_vaccinations.reset_index(drop=True, inplace=True)\n",
        "  #print(df_country_total_vaccinations.head(10))\n",
        "\n",
        "  # Apply normalisation to 'total_vaccinations'\n",
        "  df_country_total_vaccinations[\"total_vaccinations\"] = df_country_total_vaccinations[\"total_vaccinations\"].apply(df_normalise, uk_min=min_total_vaccinations_uk)\n",
        "  print(df_country_total_vaccinations.head(10))\n",
        "\n",
        "  # 4. Display the countries for which total vaccinated is at the same rate or more than the UK\n",
        "  #df_compare is of type DataFrameGroupBy\n",
        "  df_compare = df_country_total_vaccinations.groupby(\"total_vaccinations\") #(\"country\")\n",
        "  \n",
        "  #df_compare.sort_values(by=[\"total_vaccinations\"], ascending=[False], inplace=True)\n",
        "  print(df_compare.head(10))\n",
        "\n",
        "  print(f'Iterating through df_compare\\n')\n",
        "  df1 = df_compare.get_group(1)\n",
        "  print(df1['country'].unique())\n",
        "    \n",
        "\n",
        "normalise_total_vaccinations()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}